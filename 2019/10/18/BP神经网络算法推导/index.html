<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title> BP神经网络算法推导 | DongMing Tech</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script></head></html><body><main class="content"><section class="outer"><article id="post-BP神经网络算法推导" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name"> BP神经网络算法推导</h1></header><div class="article-meta"> <a href="/2019/10/18/BP神经网络算法推导/" class="article-date"><time datetime="2019-10-18T08:21:13.000Z" itemprop="datePublished">2019-10-18</time></a></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h1 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h1><p>BP神经网络是一种多层前馈神经网络，信号前向传播，误差反向传播，采用误差反向传播算法。<br>BP算法以误差平方为目标函数，采用梯度下降计算目标函数的最小值。<br>本文主要记录BP算法的数学推导过程，仅供备忘，完全参考：<a href="https://blog.csdn.net/qq_32865355/article/details/80260212" target="_blank" rel="noopener">https://blog.csdn.net/qq_32865355/article/details/80260212</a></p><p>BP神经网络主要分为两个过程：<br>1.数据前向传播，依次经过输入层，隐含层，最后到达输出层<br>2.误差反向传播，从输出层到隐含层，最后到达输入层，并依次调节各层的连接权重及偏置。</p><h1 id="记号说明"><a href="#记号说明" class="headerlink" title="记号说明"></a>记号说明</h1><ul><li>$n_l$:第l层的神经元的个数</li><li>$f(.)$:神经元激活函数</li><li>$W^{(l)}$:第l-1层到l层的权重矩阵</li><li>$w_{ij}^{(l)}$:第l-j层中的第j个神经元到第l层中的i个神经元的连接权重</li><li>$b^{(l)}$:第l-1层到l层的偏置</li><li>$z^{(l)}$:第l层的神经元的状态</li><li>$a^{(l)}$:第l层的神经元的输出值</li></ul><h1 id="BP三层神经网络结构"><a href="#BP三层神经网络结构" class="headerlink" title="BP三层神经网络结构:"></a>BP三层神经网络结构:</h1><p><img src="/2019/10/18/BP神经网络算法推导/bp.jpg" alt></p><p>输入数据：$X=(x_1,x_2,x_3)^T$<br>一个隐藏层，其有三个节点<br>一个输出层，其有两个节点</p><h1 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h1><p>第二层神经元的状态及输出值分别：</p><ul><li>$z_1^{(2)}=w_{11}^{(2)}x_1+w_{12}^{(2)}x_2+w_{13}^{(2)}x_3+b_{1}^{(2)}$</li><li>$z_2^{(2)}=w_{21}^{(2)}x_1+w_{22}^{(2)}x_2+w_{23}^{(2)}x_3+b_{2}^{(2)}$</li><li>$z_3^{(2)}=w_{31}^{(2)}x_1+w_{32}^{(2)}x_2+w_{33}^{(2)}x_3+b_{3}^{(2)}$</li><li>$a_1^{(2)}=f(z_1^{(2)})$</li><li>$a_2^{(2)}=f(z_2^{(2)})$</li><li>$a_3^{(2)}=f(z_3^{(2)})$</li></ul><p>第三层神经元的状态及输出值分别：</p><ul><li>$z_1^{(3)}=w_{11}^{(3)}x_1+w_{12}^{(3)}x_2+w_{13}^{(3)}x_3+b_{1}^{(3)}$</li><li>$z_2^{(3)}=w_{21}^{(3)}x_1+w_{22}^{(3)}x_2+w_{23}^{(3)}x_3+b_{2}^{(3)}$</li><li>$a_1^{(3)}=f(z_1^{(3)})$</li><li>$a_2^{(3)}=f(z_2^{(3)})$</li></ul><p>可得：</p><ul><li>$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$</li><li>$a^{(l)}=f(z^{(l)})$</li><li>$2 \leq l \leq L$</li></ul><h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><p>某一个训练数据$(x^{(i)},y^{(i)})$的代价函数:<br>$E_{(i)}=\frac{1}{2}\sum_{j=1}^{n}(y_j^{(i)}-o_j^{(i)})^2$</p><p>$y^{(i)}$为期望输出，$o^{(i)}$为神经网络实际输出，<br>所有训练样本的总体代价函数：<br>$E_{(total)}=\frac{1}{N}\sum_{i=1}^{N}E_{(i)}$</p><p>目标是调整各层的权重W和偏置b,使得代价函数最小</p><p>采用随机梯度下降算法，可得如下参数更新公式：<br>$\begin{align*} W^{(l)}&amp;=W^{(l)}+(-1\eta )\frac{\partial E_{total}}{\partial W^{(l)}} \\\\&amp;=W^{(l)}-\frac{\eta}{N}\sum_{i=1}^N\frac{\partial E_{(i)}}{\partial W^{(l)}} \end{align*}$<br>$\begin{align*} b^{(l)}&amp;=b^{(l)}+(-1\eta )\frac{\partial E_{total}}{\partial b^{(l)}} \\\\&amp;=b^{(l)}-\frac{\eta}{N}\sum_{i=1}^N\frac{\partial E_{(i)}}{\partial b^{(l)}} \end{align*}$</p><p>由上面公式可知，只需要求得$E_{(i)}$对参数W和b的偏导，即:</p><ul><li>$\frac{\partial E_{(i)}}{\partial W^{(l)}}$</li><li>$\frac{\partial E_{(i)}}{\partial b^{(l)}}$</li></ul><p>即可得到参数的迭代更新公式。<br>$E_{(i)}$为单个数据的误差，简记为E。</p><h1 id="输出层参数更新"><a href="#输出层参数更新" class="headerlink" title="输出层参数更新"></a>输出层参数更新</h1><p>$\begin{align*} E&amp;=\frac{1}{2}\left((y_1-a_1^{(3)})^2+(y_2-a_2^{(3)})^2\right )\\\\&amp;=\frac{1}{2}\left((y_1-f(z_1^{(3)}))^2+(y_2-f(z_2^{(3)}))^2\right )\\\\&amp;=\frac{1}{2}\left((y_1-f(w_{11}^{(3)}a_{1}^{(2)}+w_{12}^{(3)}a_{2}^{(2)}+w_{13}^{(3)}a_{3}^{(2)}+b_1^{(3)}))^2+(y_2-f(w_{21}^{(3)}a_{1}^{(2)}+w_{22}^{(3)}a_{2}^{(2)}+w_{23}^{(3)}a_{3}^{(2)}+b_2^{(3)}))^2\right)\end{align*}$</p><p>对连接权重参数w求偏导：<br>$\begin{align*} \frac{\partial E}{\partial w_{11}^{(3)}}&amp;=\frac{1}{2}\cdot 2(y_1-a_1^{(3)})^2(-\frac{\partial{a_1^{(3)}}}{w_{11}^{(3)}})\\\\&amp;=-(y_1-a_1^{(3)})f’(z_1^{(3)})\frac{\partial{z_1^{(3)}}}{w_{11}^{(3)}}\\\\&amp;=-(y_1-a_1^{(3)})f’(z_1^{(3)})a_1^{(2)}\end{align*}$<br>定义:<br>$\delta _i^{(l)}=\frac{\partial E}{\partial z_i^{(l)}}$</p><p>则有:<br>$\delta _1^{(3)}=\frac{\delta E}{\partial z_1^{(3)}}=\frac{\partial E}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}=-(y_1-a_1^{(3)})f’(z_1^{(3)})$</p><p>于是:<br>$\frac{\partial E}{\partial w_{11}^{(3)}}=\frac{\partial E}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial w_{11}^{(3)}}=-(y_1-a_1^{(3)})f’(z_1^{(3)})a_1^{(2)}=\delta_1^{(3)}a_1^{(2)}$</p><p>同理:</p><ul><li>$\frac{\partial E}{\partial w_{12}^{(3)}}=\partial_1^{(3)}a_2^{(2)}$</li><li>$\frac{\partial E}{\partial w_{13}^{(3)}}=\partial_1^{(3)}a_3^{(2)}$</li><li>$\frac{\partial E}{\partial w_{21}^{(3)}}=\partial_1^{(2)}a_1^{(2)}$</li><li>$\frac{\partial E}{\partial w_{22}^{(3)}}=\partial_1^{(2)}a_2^{(2)}$</li><li>$\frac{\partial E}{\partial w_{23}^{(3)}}=\partial_1^{(2)}a_3^{(2)}$</li></ul><p>推广到一般情况，假设神经网络共L层，则有：<br>$\begin{align*}&amp;\delta_i^{(l)}=-(y_i-a^{(L)})f’(z_i^{(L)}) \quad (1 \leq i \leq n) \\\\&amp; \frac{\partial E}{\partial {w_{ij}^{(L)}}}=\delta_i^{(L)}a_j^{(L-1)} \quad (1 \leq i \leq n) (1 \leq j \leq n-1)\end{align*}$</p><h1 id="隐藏层参数更新"><a href="#隐藏层参数更新" class="headerlink" title="隐藏层参数更新"></a>隐藏层参数更新</h1><p>对隐藏层神经元的权重参数求偏导，根据$\delta _i^{(l)}$的定义，有:<br>$\begin{align*}\frac{\partial E}{\partial w_{ij}^{(l)}}&amp;=\frac{\partial E}{\partial z_i^{(l)}}\frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}} \\\\&amp;=\delta_i^{(l)}\frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}} \\\\&amp;=\delta_i^{(l)}a_j^{(l-1)}\end{align*}$</p><p>其中$\delta_i^{(l)}$推导如下:<br>$\begin{align*}\delta_i^{(l)}&amp;=\frac{\partial E}{\partial z_i^{(l)}} \\\\&amp;=\sum_{j=1}^{n}\frac{\partial E}{\partial z_j^{(l+1)}}\frac{\partial z_j^{(l+1)}}{\partial z_{i}^{(l)}} \\\\&amp;=\sum_{j=1}^{n}\delta_j^{(l+1)}\frac{\partial z_j^{(l+1)}}{\partial z_{i}^{(l)}}\end{align*}$</p><p>其中:<br>$\frac{\partial E}{\partial z_i^{(l)}}=\frac{\partial E}{\partial z_1^{(l+1)}}\frac{\partial z_1^{(l+1)}}{\partial z_{i}^{(l)}}+\frac{\partial E}{\partial z_2^{(l+1)}}\frac{\partial z_2^{(l+1)}}{\partial z_{i}^{(l)}}+\cdots+\frac{\partial E}{\partial z_n_{l+1}^{(l+1)}}\frac{\partial z_n_{l+1}^{(l+1)}}{\partial z_{i}^{(l)}}=\sum_{j=1}^{n_{l+1}}\frac{\partial E}{\partial z_j^{(l+1)}}\frac{\partial z_j^{(l+1)}}{\partial z_{i}^{(l)}}$</p><p>因为:$z_j^{(l+1)}=\sum_{i=1}^{n_l}w_{ji}^{(l+1)}a_j^{(l)}+b_j^{(l+1)}=\sum_{i=1}^{n_l}w_{ji}^{(l+1)}f(z_j^{(l)})+b_j^{(l+1)}$</p><p>所以:$\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}=\frac{\partial z_j^{(l+1)}}{\partial a_i^{(l)}}\frac{\partial a_i^{(l)}}{\partial z_i^{(l)}}=w_{ji}^{(l+1)}f’(z_i^{(l)})$</p><p>代入$\delta_i^{(l)}$中，则有:<br>$\delta_i^{(l)}=\sum_{j=1}^{n_{l+1}}\delta_i^{(l+1)}w_{ji}^{(l+1)}f’(z_i^{(l)})=\left(\sum_{j=1}^{n_{l+1}}\delta_i^{(l+1)}w_{ji}^{(l+1)}\right)f’(z_i^{(l)})$</p><p>用向量的形式表示:<br>$\delta^{(l)}=\left((W^{(l+1)})^T\delta^{(l+1)}\right)\bigodot f’(z_i^{(l)})$</p><p>上述公式是BP的核心公式，即利用l+1层的$\delta^{(l+1)}$来计算l层的$\delta^{(l)}$</p><h1 id="输出层和隐藏层的偏置参数更新"><a href="#输出层和隐藏层的偏置参数更新" class="headerlink" title="输出层和隐藏层的偏置参数更新"></a>输出层和隐藏层的偏置参数更新</h1><p>$\frac{\partial E}{\partial b_i^{(l)}}=\frac{\partial E}{\partial z_i^{(l)}}\frac{\partial z_i^{(l)}}{\partial b_i^{(l)}}=\delta_i^{(l)}$</p></div><footer class="article-footer"> <a data-url="https://dongmingtech.com/2019/10/18/BP神经网络算法推导/" data-id="ck21pe15b0000c4lm2mnr2rsq" class="article-share-link">分享</a><ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ai/">ai</a></li></ul></footer><i class="fe fe-bar-chart"></i> <span class="post-count">1.5k</span>字 &emsp;<i class="fe fe-clock"></i> <span class="post-count">7</span>分钟</div><nav class="article-nav"> <a href="/2019/10/17/Hexo中支持MathJAX/" class="article-nav-link"><strong class="article-nav-caption">后一篇</strong><div class="article-nav-title">Hexo中支持MathJAX</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"9d46fabb0c2dcd2cc3f2",clientSecret:"aa1cb25a344c0e97d83609a255dbcdc43a358282",repo:"blogposts",owner:"cfreebuf",admin:["cfreebuf"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by"> &emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">访客数:<span id="busuanzi_value_site_uv"></span></span>&emsp;</div></div><ul class="list-inline"><ul class="list-inline"><li>全站共<span class="post-count">3.9k</span>字</li></ul><li>&copy; 2019 DongMing Tech</li><li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li><li>Theme <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li></ul></div></footer></main><aside class="sidebar sidebar-specter"> <button class="navbar-toggle"></button><nav class="navbar"><div class="logo"> <a href="/"><img src="/images/main.svg" alt="DongMing Tech"></a></div><ul class="nav nav-main"><li class="nav-item"> <a class="nav-item-link" href="/">主页</a></li><li class="nav-item"> <a class="nav-item-link" href="/archives">归档</a></li><li class="nav-item"> <a class="nav-item-link" href="/gallery">相册</a></li><li class="nav-item"> <a class="nav-item-link" href="/about">关于</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> 搜索</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop"><i class="fe fe-rocket"></i></div></li><li class="nav-item"><a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed"><i class="fe fe-feed"></i></a></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"> <input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></body>